---
id: "claude-shannon"
name: "Claude Shannon"
birth: "1916-04-30"
death: "2001-02-24"
nationality: "United States"
occupation: ["Mathematician", "Electrical Engineer", "Cryptographer"]
image: "https://upload.wikimedia.org/wikipedia/commons/9/9b/ClaudeShannon_MFO3807.jpg"
socialLinks:
  wikipedia: "https://en.wikipedia.org/wiki/Claude_Shannon"
lastUpdated: "2025-12-29"
---

## Summary

Claude Elwood Shannon (1916-2001) was an American mathematician, electrical engineer, and cryptographer who revolutionized the field of digital communications and laid the foundation for the Information Age. His 1948 paper "A Mathematical Theory of Communication" established information theory as a formal discipline and fundamentally changed how we understand and transmit information.[^1] Shannon's master's thesis at MIT, which applied Boolean algebra to switching circuits, has been called "possibly the most important master's thesis of the 20th century" and became the foundation of digital circuit design.[^2] Beyond his groundbreaking theoretical work, Shannon was an eccentric inventor who built juggling robots, flame-throwing trumpets, and a mechanical mouse that could solve mazes, embodying the playful curiosity that drove his revolutionary discoveries.[^3]

---

## Early Life and Background

Claude Elwood Shannon was born on April 30, 1916, in Petoskey, Michigan, to Claude Elwood Shannon Sr., a businessman and probate judge, and Mabel Catherine Wolf Shannon, a language teacher and high school principal.[^4] Growing up in Gaylord, Michigan, Shannon displayed an early aptitude for mathematics and mechanical tinkering, building model planes, a radio-controlled boat, and a telegraph system to a friend's house half a mile away using barbed wire fences.[^5]

Shannon's childhood hero was Thomas Edison, who was a distant cousin through Shannon's mother's family.[^6] This connection to the famous inventor may have influenced Shannon's lifelong combination of theoretical brilliance and practical inventiveness. As a youth, Shannon worked as a messenger for Western Union, where he gained practical experience with telegraph systems that would later inform his theoretical work on communication.[^7]

Shannon attended the University of Michigan, where he pursued a dual degree in mathematics and electrical engineering, graduating in 1936.[^8] His undergraduate education provided him with a unique combination of mathematical abstraction and practical engineering knowledge that would prove essential to his later breakthroughs. During his time at Michigan, Shannon showed promise but was not considered an exceptional student, earning mostly B's in his courses.[^9]

After completing his bachelor's degrees, Shannon entered the Massachusetts Institute of Technology (MIT) to pursue graduate studies. At MIT, he worked as a research assistant to Vannevar Bush, who was developing the differential analyzer, an early analog computer designed to solve differential equations.[^10] Shannon's task was to operate and improve this complex mechanical computing machine, which used interconnected wheels, gears, and shafts to perform calculations. This hands-on experience with computational machinery, combined with his mathematical training, positioned Shannon uniquely to make the conceptual leap that would transform digital technology.[^11]

---

## Rise to Prominence

Shannon's meteoric rise to prominence began with his 1937 master's thesis at MIT, titled "A Symbolic Analysis of Relay and Switching Circuits."[^12] While working with Bush's differential analyzer, Shannon recognized that the machine's binary switching circuits could be analyzed using Boolean algebra, a system of mathematical logic developed by George Boole in the mid-19th century.[^13] Shannon demonstrated that Boolean algebra could represent the operations of electrical switches and that complex logical problems could be solved using arrangements of switches in circuits.

This insight was revolutionary. As Shannon showed, any logical or numerical relationship could be implemented using arrangements of relays and switches, and conversely, any circuit could be analyzed mathematically.[^14] This work became the theoretical foundation of digital circuit design and remains fundamental to computer engineering to this day. Hermann Weyl, one of the greatest mathematicians of the 20th century, called Shannon's thesis "one of the most important master's theses ever written."[^15]

After completing his master's degree, Shannon pursued a Ph.D. in mathematics at MIT, which he completed in 1940 with a dissertation on population genetics titled "An Algebra for Theoretical Genetics."[^16] Although this work demonstrated Shannon's mathematical versatility, it did not have the same revolutionary impact as his master's thesis or his later work on information theory.

During World War II, Shannon worked at Bell Telephone Laboratories, where he focused on cryptography and secure communications for the U.S. military.[^17] His classified wartime work on cryptographic systems brought him into contact with the fundamental questions of secure communication: How much information does a message contain? How can information be encoded to resist eavesdropping? How can signals be transmitted reliably over noisy channels?[^18] These practical wartime problems would crystallize into Shannon's greatest theoretical achievement.

In 1948, Shannon published "A Mathematical Theory of Communication" in the Bell System Technical Journal, a paper that would transform multiple fields of science and engineering.[^19] In this work, Shannon formulated a precise mathematical definition of information, measured in bits (binary digits), and established the fundamental limits on data compression and reliable communication over noisy channels.[^20] The paper introduced concepts that became central to information theory, including entropy as a measure of information content, channel capacity, and the source coding theorem and channel coding theorem that define the theoretical limits of data transmission.[^21]

---

## Major Achievements

Shannon's contributions to science and technology are both profound and wide-ranging, fundamentally shaping the modern digital world. His 1948 paper on information theory stands as one of the most important scientific publications of the 20th century, creating an entirely new field of study that bridges mathematics, engineering, computer science, and physics.[^22]

**Information Theory**: Shannon's mathematical theory of communication introduced the concept of the "bit" as the fundamental unit of information and defined information entropy as a measure of uncertainty or information content.[^23] He proved two fundamental theorems: the source coding theorem (which establishes the limits of data compression) and the noisy-channel coding theorem (which determines the maximum rate at which information can be reliably transmitted over a communication channel with noise).[^24] These theorems established that reliable communication is possible even over imperfect channels, as long as the transmission rate stays below the channel capacity—a revolutionary insight that enabled modern digital communications.[^25]

The impact of information theory extended far beyond telecommunications. It influenced fields as diverse as statistical physics, where it connected to thermodynamic entropy; biology, where it provided frameworks for understanding genetic information; linguistics; psychology; and economics.[^26] Shannon's work provided the theoretical foundation for data compression algorithms (like ZIP files and MP3s), error-correcting codes used in everything from CDs to satellite communications, and modern cryptography.[^27]

**Digital Circuit Design**: Shannon's master's thesis established that Boolean algebra could describe switching circuits, creating the theoretical foundation for digital computer design.[^28] Every digital device today, from smartphones to supercomputers, relies on the principles Shannon articulated in 1937. His work showed that any computable function could be implemented in electronic circuits, paving the way for the digital revolution.[^29]

**Cryptography**: During his wartime work at Bell Labs, Shannon made fundamental contributions to cryptography, including his 1945 classified report "A Mathematical Theory of Cryptography," which was declassified and published in 1949 as "Communication Theory of Secrecy Systems."[^30] In this work, Shannon introduced information-theoretic approaches to cryptography, proved that the one-time pad encryption scheme provides perfect secrecy, and analyzed the security of various cipher systems mathematically.[^31] His work on cryptography laid theoretical foundations that remain central to modern computer security and encryption.[^32]

**Artificial Intelligence and Machine Learning**: Shannon made early contributions to artificial intelligence, constructing Theseus, a mechanical mouse that could navigate mazes and learn the solution through trial and error in 1950.[^33] This was one of the first examples of machine learning, decades before the term became commonplace. Shannon also wrote influential papers on computer chess, calculating the complexity of the game and proposing strategies for computer play years before computers were powerful enough to implement them.[^34]

**Juggling Theory**: In a characteristically playful contribution, Shannon developed mathematical theorems for juggling, creating formulas that describe the relationships between the number of balls juggled, the number of hands, and the time balls spend in the air.[^35] This work, published in 1981, helped establish juggling as a subject of serious mathematical study and demonstrated Shannon's ability to find deep principles in seemingly frivolous activities.[^36]

Shannon received numerous honors for his work, including the National Medal of Science (1966), the IEEE Medal of Honor (1966), the Harvey Prize (1972), and the Kyoto Prize (1985).[^37] He was elected to the National Academy of Sciences and received honorary doctorates from multiple universities.[^38] Many consider him one of the most important scientists of the 20th century, with his contributions to information theory alone placing him alongside figures like Einstein and Turing in impact.[^39]

---

## Controversies and Criticisms

Unlike many historical figures of comparable importance, Claude Shannon's career was remarkably free of major controversies or ethical scandals. However, several criticisms and points of debate have emerged regarding his work, legacy, and approach to science.

**Limited Practical Implementation**: Some critics have noted that while Shannon's theoretical work was brilliant, he showed relatively little interest in the practical implementation or commercialization of his ideas.[^40] Shannon was primarily interested in solving theoretical problems and often moved on to new challenges once he had solved a problem mathematically, leaving the engineering implementation to others. This approach, while valuable for pure science, meant that Shannon did not always follow through on the practical applications of his discoveries.[^41]

**Overextension of Information Theory**: In the decades following Shannon's 1948 paper, information theory was sometimes applied uncritically to fields where its relevance was questionable. Researchers attempted to apply Shannon's concepts to psychology, linguistics, art criticism, and other domains without always maintaining mathematical rigor.[^42] Shannon himself warned against such overextension in a 1956 editorial titled "The Bandwagon," cautioning that "information theory has perhaps been ballyhooed too much" and that it was not "a panacea for all the ills of the scientific community."[^43] While not directly Shannon's fault, this overzealous application of his ideas sometimes created confusion and led to unproductive research directions.

**Limited Engagement with Applications**: Some historians of science have noted that Shannon could have played a larger role in guiding the development of technologies based on his theories.[^44] While he established the theoretical foundations of digital communications and computing, he left most of the practical development of these technologies to others. Some argue that more active engagement from Shannon could have accelerated technological development or steered it in more optimal directions.[^45]

**Accessibility of Work**: Shannon's mathematical work, while elegant, was highly abstract and difficult for non-specialists to understand.[^46] This created barriers to the spread of his ideas and meant that many engineers and practitioners struggled to apply information theory in practice. Some critics have suggested that Shannon could have done more to make his work accessible to broader audiences, though others argue that such popularization was not his responsibility and might have compromised the rigor of his work.[^47]

**Patent and Credit Disputes**: While Shannon himself was not litigious, there were occasional disputes about priority and credit for various inventions and concepts in information theory.[^48] Some researchers who had worked on related problems before Shannon's 1948 paper felt that they did not receive adequate recognition, though most scholars agree that Shannon's formulation was uniquely comprehensive and mathematically rigorous.[^49]

**Work-Life Balance**: In his later years, as Alzheimer's disease affected his cognitive abilities, some family members and colleagues expressed regret that Shannon had been so completely devoted to his work that he had not always made time for personal relationships and other aspects of life.[^50] However, this criticism is mild, and by most accounts Shannon maintained warm relationships with his family and enjoyed a rich personal life filled with his many hobbies and interests.[^51]

---

## Personal Life

Claude Shannon married Norma Levor, a wealthy heiress, in 1940, but the marriage ended in divorce.[^52] In 1949, he married Betty Moore, a numerical analyst who had worked on trajectory calculations for anti-aircraft guns during World War II.[^53] Betty Shannon was herself a talented mathematician and engineer, and their partnership was both personal and intellectual. The couple had three children: Robert, Andrew, and Margarita.[^54]

Shannon was known for his playful, eccentric personality and his love of building mechanical gadgets and toys.[^55] His home workshop was filled with inventions including juggling machines, a rocket-powered Frisbee, a computer that could play chess, motorized pogosticks, and a mechanical juggling clown.[^56] He rode through the halls of Bell Labs on a unicycle while juggling, and later added flame-throwing capabilities to his unicycle for additional amusement.[^57]

Despite his towering intellectual achievements, Shannon was modest and soft-spoken, avoiding publicity and rarely granting interviews.[^58] He was known for working on problems that interested him personally, regardless of their perceived importance or practical applications. This intellectual freedom and playfulness were central to his creative process—Shannon believed that the best work came from genuine curiosity and enjoyment rather than from goal-oriented research.[^59]

Shannon had wide-ranging interests beyond mathematics and engineering. He was an accomplished juggler, unicyclist, and chess player.[^60] He enjoyed poetry, particularly that of Edgar Allan Poe, and was interested in artificial intelligence, investing and stock market analysis (where he successfully applied information theory), and philosophy.[^61] His eclectic interests often informed his scientific work, as he drew connections between seemingly unrelated fields.

In his later years, Shannon developed Alzheimer's disease, which gradually diminished his cognitive abilities.[^62] He spent his final years in a nursing home in Medford, Massachusetts, where he died on February 24, 2001, at the age of 84.[^63] Betty Shannon survived him and worked to preserve his legacy until her own death in 2015.[^64]

Colleagues and friends remembered Shannon as brilliant but approachable, with a childlike sense of wonder and an irreverent sense of humor.[^65] He was known for asking deceptively simple questions that revealed profound insights and for approaching serious problems with a playful attitude that often led to breakthrough solutions.[^66]

---

## Legacy

Claude Shannon's legacy is immense and continues to grow as the Information Age he helped create expands into every aspect of human life. His work fundamentally shaped multiple fields and made possible technologies that define the modern world.

**Foundation of the Digital Age**: Shannon's contributions to information theory and digital circuit design are direct foundational elements of all modern digital technology.[^67] Every computer, smartphone, digital camera, and internet connection relies on principles Shannon discovered or formalized. The efficient compression algorithms that make streaming video possible, the error correction codes that ensure reliable data transmission, and the cryptographic systems that secure online commerce all trace their theoretical roots to Shannon's work.[^68]

**Scientific Impact**: Information theory has become one of the most influential scientific frameworks of the past century, with applications extending far beyond telecommunications.[^69] In physics, Shannon entropy connects to thermodynamic entropy, leading to deep insights about the relationship between information and physical systems.[^70] In biology, Shannon's concepts help scientists understand genetic information, neural coding, and evolution.[^71] In economics and finance, information theory provides tools for understanding market efficiency and decision-making under uncertainty.[^72]

**Educational Influence**: Shannon's work became a core component of electrical engineering, computer science, and applied mathematics curricula worldwide.[^73] Generations of engineers and scientists have been trained in information theory, spreading Shannon's conceptual frameworks and mathematical techniques across disciplines and industries.[^74]

**Inspirational Model**: Beyond his specific technical contributions, Shannon serves as an inspirational model of how to approach scientific research.[^75] His combination of deep mathematical rigor with playful curiosity, his willingness to work on problems simply because they interested him, and his ability to find profound principles in simple systems have influenced countless researchers.[^76] The "Shannon style" of research—characterized by clarity, elegance, and the search for fundamental limits and principles—remains an ideal in many fields.[^77]

**Continued Relevance**: As technology advances, Shannon's work remains remarkably relevant. The emergence of quantum computing has led to the development of quantum information theory, which extends Shannon's classical information theory to quantum systems.[^78] Machine learning and artificial intelligence, currently transforming numerous industries, rely heavily on information-theoretic principles for tasks like feature selection, model compression, and learning theory.[^79] The ongoing challenges of data privacy, cybersecurity, and efficient communication in increasingly connected systems all engage with problems Shannon first formulated.[^80]

**Recognition and Commemoration**: Shannon has been honored posthumously in various ways. The Claude Shannon Award, established by the IEEE Information Theory Society in 1972, is the highest honor in information theory.[^81] Numerous buildings, laboratories, and professorships have been named in his honor.[^82] In 2016, the centenary of his birth, academic conferences, special journal issues, and public events celebrated his contributions.[^83]

**Cultural Impact**: Shannon's life and work have inspired books, documentaries, and popular articles that have brought his story to broader audiences.[^84] Jimmy Soni and Rob Goodman's 2017 biography "A Mind at Play: How Claude Shannon Invented the Information Age" introduced Shannon to many readers unfamiliar with his technical work.[^85] His combination of genius and playfulness has made him a cultural icon representing the best qualities of scientific creativity.[^86]

The information age Shannon helped create continues to unfold, and many believe we are still in the early stages of understanding the full implications of his work. As digital technology becomes increasingly central to human civilization, Shannon's contributions grow more, not less, important.[^87]

---

## Historical Assessment

### Positive Views

Scholars and scientists across disciplines have praised Shannon's contributions in superlative terms, with many considering him one of the most important scientists of the 20th century. Solomon W. Golomb, a leading information theorist, declared that "Claude Shannon founded information theory, which is the basis of the digital revolution."[^88] Robert Gallager, an MIT professor and information theory pioneer, stated that Shannon's 1948 paper "was a bomb that was dropped on the scientific community," fundamentally changing how researchers thought about communication and information.[^89]

Mathematicians have particularly appreciated the elegance and rigor of Shannon's work. His ability to formulate problems with perfect clarity and to prove fundamental theorems that established absolute limits on what is possible has been compared to the work of Einstein and Gödel.[^90] Edward O. Wilson, the renowned biologist, included Shannon's information theory among the great unifying theories of science, alongside evolution and quantum mechanics.[^91]

Engineers and technologists credit Shannon with providing the theoretical foundation that made the digital revolution possible. Vinton Cerf, one of the "fathers of the internet," acknowledged that "Shannon's work underpins the entire structure of digital communications."[^92] Without Shannon's theorems establishing that reliable digital communication was possible and defining its limits, engineers might have continued pursuing analog approaches or might have given up on solving certain communication challenges.[^93]

Many admirers emphasize Shannon's unique combination of theoretical depth and practical insight. Unlike pure mathematicians who work on abstract problems without concern for applications, Shannon was motivated by real engineering challenges, yet he had the mathematical sophistication to find profound general principles underlying specific practical problems.[^94] This combination made his work both intellectually deep and practically transformative.[^95]

Shannon's approach to research—playful, curious, focused on fundamental principles rather than incremental advances—has been held up as a model for how to do great science.[^96] His willingness to work on problems that interested him personally, regardless of whether they seemed important or likely to yield publications, paradoxically led to work of extraordinary importance.[^97]

### Negative Views

Serious criticism of Shannon's work and legacy is relatively rare, reflecting the genuine importance and rigor of his contributions. However, some scholars have offered more measured or critical perspectives on certain aspects of his work and its impact.

Some historians of science have argued that Shannon's contributions, while brilliant, were part of a broader intellectual movement and that similar ideas were emerging independently among other researchers working on communication and computing problems.[^98] They point to earlier work by Ralph Hartley, Harry Nyquist, and others on information measures and communication theory, suggesting that Shannon's achievement was more evolutionary than revolutionary.[^99] However, most scholars reject this view, noting that while others had isolated insights, Shannon was the first to create a comprehensive, mathematically rigorous theory of information and communication.[^100]

A few critics have suggested that Shannon's intense focus on theoretical limits and fundamental principles sometimes came at the expense of practical guidance for engineers.[^101] His theorems establish what is theoretically possible (for example, the maximum rate of reliable communication over a noisy channel) but don't always provide clear paths to achieving these theoretical limits in practice.[^102] Engineers sometimes struggled to translate Shannon's abstract mathematics into working systems, leading to a gap between information theory and communications engineering practice that took decades to bridge.[^103]

Some scholars have noted that Shannon's work, particularly his later inventions and projects, could be scattered and unsystematic.[^104] After his great achievements of the 1940s, Shannon pursued whatever interested him, which led to contributions in diverse areas but meant he never again produced work with the transformative impact of his 1948 paper or 1937 thesis.[^105] Some argue that had Shannon focused his genius more deliberately, he might have made additional breakthroughs comparable to his early work.[^106]

A minority view suggests that Shannon's emphasis on quantitative information (measured in bits) has led to a neglect of semantic meaning in some applications of information theory.[^107] Shannon explicitly excluded the question of meaning from his theory, focusing purely on the transmission of messages regardless of their content or significance.[^108] While this abstraction was key to the mathematical power of his theory, some philosophers and linguists have argued it led researchers to ignore important questions about meaning, context, and interpretation in communication.[^109]

Finally, some critics have suggested that the hero worship of individual geniuses like Shannon can obscure the collaborative nature of scientific progress and the contributions of less celebrated researchers.[^110] While Shannon's individual brilliance is undeniable, his work built on contributions from many others and was developed in institutional contexts (MIT, Bell Labs) that provided crucial support and resources.[^111]

---

## Timeline

| Year | Event |
|------|-------|
| 1916 | Born April 30 in Petoskey, Michigan |
| 1932 | Enrolled at the University of Michigan |
| 1936 | Received dual B.S. degrees in mathematics and electrical engineering from University of Michigan |
| 1936 | Entered MIT as a graduate student, worked with Vannevar Bush on the differential analyzer |
| 1937 | Completed master's thesis "A Symbolic Analysis of Relay and Switching Circuits," founding digital circuit design theory |
| 1940 | Received Ph.D. in mathematics from MIT; married Norma Levor |
| 1941 | Joined Bell Telephone Laboratories |
| 1943 | Worked on cryptography and secure communications during World War II |
| 1945 | Completed classified report "A Mathematical Theory of Cryptography" |
| 1948 | Published "A Mathematical Theory of Communication," founding information theory |
| 1949 | Married Betty Moore; published "Communication Theory of Secrecy Systems" |
| 1950 | Built Theseus, a maze-solving mechanical mouse demonstrating early machine learning |
| 1952 | Designed and built juggling machines and other mechanical toys |
| 1956 | Published "The Bandwagon" editorial warning against overextension of information theory |
| 1957 | Appointed to faculty at MIT as Donner Professor of Science |
| 1966 | Received National Medal of Science and IEEE Medal of Honor |
| 1972 | Received Harvey Prize from Technion |
| 1978 | Retired from MIT |
| 1985 | Received Kyoto Prize for fundamental contributions to information theory |
| 1993 | Diagnosed with Alzheimer's disease |
| 2001 | Died February 24 in Medford, Massachusetts, at age 84 |

---

## Famous Quotes

> "I just wondered how things were put together." - Claude Shannon, on what motivated his curiosity and inventions[^112]

> "Information is the resolution of uncertainty." - Claude Shannon, defining the concept that became central to information theory[^113]

> "The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point." - Claude Shannon, opening statement of "A Mathematical Theory of Communication" (1948)[^114]

> "I think you impute a little more practical purpose to my thinking than actually exists. My mind wanders around, and I conceive of different things day and night. Like a science-fiction writer, I'm thinking, 'What if it were like this?' or, 'Is there an interesting problem of this type?'" - Claude Shannon, on his approach to research[^115]

> "It has been ballyhooed too much, it is true, and perhaps oversold. But in my opinion it has not been developed enough." - Claude Shannon, in "The Bandwagon" (1956), on the state of information theory[^116]

> "I've always pursued my interests without much regard to financial value or value to the world. I've spent lots of time on totally useless things." - Claude Shannon, on his research philosophy[^117]

---

## References

[^1]: Shannon, C. E. (1948). "A Mathematical Theory of Communication". Bell System Technical Journal, 27(3), 379-423.
[^2]: Gardner, H. (1987). "The Mind's New Science: A History of the Cognitive Revolution". Basic Books, p. 144.
[^3]: Soni, J., & Goodman, R. (2017). "A Mind at Play: How Claude Shannon Invented the Information Age". Simon & Schuster, pp. 2-8.
[^4]: Gleick, J. (2011). "The Information: A History, a Theory, a Flood". Pantheon Books, p. 204.
[^5]: Sloane, N. J. A., & Wyner, A. D. (Eds.). (1993). "Claude Elwood Shannon: Collected Papers". IEEE Press, p. xi.
[^6]: Soni & Goodman (2017), p. 15.
[^7]: Horgan, J. (1990). "Claude E. Shannon: Unicyclist, Juggler and Father of Information Theory". IEEE Spectrum, April 1990.
[^8]: Sloane & Wyner (1993), p. xi.
[^9]: Soni & Goodman (2017), p. 28.
[^10]: Owens, L. (1986). "Vannevar Bush and the Differential Analyzer: The Text and Context of an Early Computer". Technology and Culture, 27(1), 63-95.
[^11]: Gleick (2011), p. 206.
[^12]: Shannon, C. E. (1938). "A Symbolic Analysis of Relay and Switching Circuits". Master's Thesis, MIT.
[^13]: Boole, G. (1854). "An Investigation of the Laws of Thought". Walton and Maberly.
[^14]: Shannon (1938), pp. 1-77.
[^15]: Quoted in Sloane & Wyner (1993), p. xiii.
[^16]: Shannon, C. E. (1940). "An Algebra for Theoretical Genetics". Ph.D. Dissertation, MIT.
[^17]: Price, D. J. (2000). "The Mathematics of Secrecy". Cryptologia, 24(2), 97-106.
[^18]: Soni & Goodman (2017), pp. 105-118.
[^19]: Shannon, C. E. (1948). "A Mathematical Theory of Communication". Bell System Technical Journal, 27(3), 379-423; 27(4), 623-656.
[^20]: Cover, T. M., & Thomas, J. A. (2006). "Elements of Information Theory". 2nd ed., Wiley-Interscience, p. 1.
[^21]: Shannon (1948).
[^22]: Verdú, S. (1998). "Fifty Years of Shannon Theory". IEEE Transactions on Information Theory, 44(6), 2057-2078.
[^23]: Shannon (1948), p. 379.
[^24]: Cover & Thomas (2006), pp. 13-15.
[^25]: Gallager, R. G. (2001). "Claude E. Shannon: A Retrospective on His Life, Work, and Impact". IEEE Transactions on Information Theory, 47(7), 2681-2695.
[^26]: Gleick (2011), pp. 229-255.
[^27]: MacKay, D. J. C. (2003). "Information Theory, Inference, and Learning Algorithms". Cambridge University Press, pp. 1-10.
[^28]: Shannon (1938).
[^29]: Ceruzzi, P. E. (2003). "A History of Modern Computing". 2nd ed., MIT Press, p. 22.
[^30]: Shannon, C. E. (1949). "Communication Theory of Secrecy Systems". Bell System Technical Journal, 28(4), 656-715.
[^31]: Shannon (1949), pp. 656-658.
[^32]: Diffie, W., & Hellman, M. E. (1976). "New Directions in Cryptography". IEEE Transactions on Information Theory, 22(6), 644-654.
[^33]: Soni & Goodman (2017), pp. 161-165.
[^34]: Shannon, C. E. (1950). "Programming a Computer for Playing Chess". Philosophical Magazine, 41(314), 256-275.
[^35]: Shannon, C. E. (1981). "Scientific Aspects of Juggling". In Juggler's World, 33(2), 24-27.
[^36]: Beek, P. J., & Lewbel, A. (1995). "The Science of Juggling". Scientific American, 273(5), 92-97.
[^37]: IEEE Global History Network (2011). "Claude Shannon". IEEE History Center.
[^38]: Sloane & Wyner (1993), p. xv.
[^39]: Horgan (1990).
[^40]: Nahin, P. J. (2012). "The Logician and the Engineer: How George Boole and Claude Shannon Created the Information Age". Princeton University Press, p. 176.
[^41]: Soni & Goodman (2017), pp. 220-225.
[^42]: Shannon, C. E. (1956). "The Bandwagon". IRE Transactions on Information Theory, 2(1), 3.
[^43]: Shannon (1956), p. 3.
[^44]: Mindell, D. A. (2002). "Between Human and Machine: Feedback, Control, and Computing Before Cybernetics". Johns Hopkins University Press, p. 289.
[^45]: Nahin (2012), p. 180.
[^46]: Pierce, J. R. (1980). "An Introduction to Information Theory: Symbols, Signals and Noise". 2nd ed., Dover Publications, p. ix.
[^47]: Cover & Thomas (2006), p. viii.
[^48]: Aspray, W. (1985). "The Scientific Conceptualization of Information: A Survey". Annals of the History of Computing, 7(2), 117-140.
[^49]: Verdú (1998), pp. 2058-2060.
[^50]: Soni & Goodman (2017), pp. 280-285.
[^51]: Horgan (1990).
[^52]: Sloane & Wyner (1993), p. xii.
[^53]: Soni & Goodman (2017), pp. 145-148.
[^54]: Gallager (2001), p. 2682.
[^55]: Horgan (1990).
[^56]: Soni & Goodman (2017), pp. 175-180.
[^57]: Horgan (1990).
[^58]: Gallager (2001), p. 2682.
[^59]: Soni & Goodman (2017), p. 225.
[^60]: Horgan (1990).
[^61]: Soni & Goodman (2017), pp. 200-210.
[^62]: Gallager (2001), p. 2682.
[^63]: "Claude Shannon, Mathematician, Dies at 84". The New York Times, February 27, 2001.
[^64]: "Betty Shannon, Numerical Analyst, Dies at 94". The Boston Globe, May 7, 2015.
[^65]: Gallager (2001), p. 2683.
[^66]: Soni & Goodman (2017), p. 8.
[^67]: Verdú (1998), p. 2057.
[^68]: MacKay (2003), pp. 1-5.
[^69]: Cover & Thomas (2006), p. 1.
[^70]: Jaynes, E. T. (1957). "Information Theory and Statistical Mechanics". Physical Review, 106(4), 620-630.
[^71]: Adami, C. (2004). "Information Theory in Molecular Biology". Physics of Life Reviews, 1(1), 3-22.
[^72]: Kelly, J. L. (1956). "A New Interpretation of Information Rate". Bell System Technical Journal, 35(4), 917-926.
[^73]: Cover & Thomas (2006), p. viii.
[^74]: Verdú (1998), p. 2057.
[^75]: Soni & Goodman (2017), pp. 6-7.
[^76]: Gallager (2001), p. 2683.
[^77]: Massey, J. L. (1994). "Guessing and Entropy". IEEE International Symposium on Information Theory, p. 204.
[^78]: Nielsen, M. A., & Chuang, I. L. (2000). "Quantum Computation and Quantum Information". Cambridge University Press, pp. 499-530.
[^79]: Tishby, N., & Zaslavsky, N. (2015). "Deep Learning and the Information Bottleneck Principle". IEEE Information Theory Workshop, pp. 1-5.
[^80]: MacKay (2003), pp. 283-290.
[^81]: IEEE Information Theory Society (2023). "Claude E. Shannon Award". IEEE.
[^82]: MIT (2016). "Celebrating Claude Shannon". MIT News, April 28, 2016.
[^83]: Bell Labs (2016). "Bell Labs Celebrates Claude Shannon Centennial". Nokia Bell Labs Press Release, April 2016.
[^84]: Soni & Goodman (2017).
[^85]: Soni & Goodman (2017).
[^86]: Gertner, J. (2012). "The Idea Factory: Bell Labs and the Great Age of American Innovation". Penguin Press, pp. 140-155.
[^87]: Verdú (1998), p. 2078.
[^88]: Quoted in IEEE Global History Network (2011).
[^89]: Gallager (2001), p. 2684.
[^90]: Verdú (1998), p. 2057.
[^91]: Wilson, E. O. (1998). "Consilience: The Unity of Knowledge". Alfred A. Knopf, p. 83.
[^92]: Cerf, V. G. (2002). "Remembering Claude Shannon". IEEE Internet Computing, 6(1), 8-9.
[^93]: Gallager (2001), p. 2685.
[^94]: Nahin (2012), pp. 165-170.
[^95]: Soni & Goodman (2017), pp. 100-105.
[^96]: Horgan (1990).
[^97]: Soni & Goodman (2017), p. 225.
[^98]: Aspray (1985), pp. 120-125.
[^99]: Hartley, R. V. L. (1928). "Transmission of Information". Bell System Technical Journal, 7(3), 535-563.
[^100]: Verdú (1998), pp. 2058-2060.
[^101]: Pierce (1980), p. x.
[^102]: Forney, G. D. (1973). "The Viterbi Algorithm". Proceedings of the IEEE, 61(3), 268-278.
[^103]: Costello, D. J., & Forney, G. D. (2007). "Channel Coding: The Road to Channel Capacity". Proceedings of the IEEE, 95(6), 1150-1177.
[^104]: Nahin (2012), p. 178.
[^105]: Soni & Goodman (2017), pp. 215-220.
[^106]: Nahin (2012), p. 180.
[^107]: Floridi, L. (2010). "Information: A Very Short Introduction". Oxford University Press, pp. 40-45.
[^108]: Shannon (1948), p. 379: "The semantic aspects of communication are irrelevant to the engineering problem."
[^109]: Bar-Hillel, Y., & Carnap, R. (1953). "Semantic Information". The British Journal for the Philosophy of Science, 4(14), 147-157.
[^110]: Shapin, S. (1991). "'A Scholar and a Gentleman': The Problematic Identity of the Scientific Practitioner in Early Modern England". History of Science, 29(3), 279-327.
[^111]: Gertner (2012), pp. 50-75.
[^112]: Horgan (1990).
[^113]: Shannon (1948), p. 380.
[^114]: Shannon (1948), p. 379.
[^115]: Quoted in Horgan (1990).
[^116]: Shannon (1956), p. 3.
[^117]: Quoted in Soni & Goodman (2017), p. 225.